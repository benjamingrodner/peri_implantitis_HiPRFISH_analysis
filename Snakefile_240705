# Snakefile

"""
Pipeline to segment hiprfish images for hsdm project

"""

# =============================================================================
# Imports
# =============================================================================
import pandas as pd
import numpy as np
import os
import sys
import glob
from collections import defaultdict
import matplotlib.pyplot as plt
import re
import yaml
from scipy.spatial.distance import squareform, pdist
from sklearn.cluster import AgglomerativeClustering
import umap
from scipy.spatial.distance import squareform, pdist, cdist
from sklearn.neighbors import NearestNeighbors

sys.path.append(config['functions_path'])
import fn_spectral_images as fsi
import image_plots as ip
import fn_hiprfish_classifier as fhc
import segmentation_func as sf
import math


# =============================================================================
# Functions
# =============================================================================

def get_input_table():
    input_table = pd.read_csv(config['input_table'])
    # input_table.columns = config['input_table_cols']
    return input_table


def get_seg_fns(date, sn, fmt):
    fns = dict_date_sn_fns[date][sn]
    M, mtype = fsi.get_ntiles(fns[0])
    seg_fns = []
    for m in range(M):
        seg_fns.append(fmt.format(date=date, sn=sn, m=m))
    return seg_fns

def get_fns(fmt):
    fns = []
    for date, dict_sn_fns in dict_date_sn_fns.items():
        for sn in dict_sn_fns.keys():
            fns.append(fmt.format(date=date, sn=sn))
    return fns

def get_czi_fns(date, sn):
    return dict_date_sn_fns[date][sn]

# =============================================================================
# Setup
# =============================================================================

# args = sys.argv
# config_fn = args[args.index("--configfile") + 1]

input_table = get_input_table()
filenames = input_table['filenames']

dict_date_sn_fns = defaultdict(lambda: defaultdict(list))
for fn in filenames:
    bn = os.path.split(fn)[1]
    date, bn = re.split('(?<=^\d{4}_\d{2}_\d{2})_', bn)
    sn, ext = re.split("(?<=fov_\d{2})", bn)
    dict_date_sn_fns[date][sn].append(fn)

out_dir = config['output_dir'] + "/{date}/{date}_{sn}"

# Segment
out_dir_seg = out_dir + "/segs"
out_dir_plot = out_dir + "/plots"
ofmt = '/{date}_{sn}_M_{m}'
seg_fmt = out_dir_seg + ofmt + "_seg.npy"
props_fmt = out_dir_seg + ofmt + "_props.csv"
plot_fmt = out_dir_plot + ofmt + "_seg_plot.png"
rgb_fmt = out_dir_plot + ofmt + "_rgb_plot.png"
segs_done_fmt = out_dir + "/snakemake_segment_done.txt"

# # Cluster
# out_fmt_clust = out_dir + "/cluster/{date}_{sn}"
# umap_fmt = out_fmt_clust + "_umap.png"
# spec_fmt = out_fmt_clust + "_spec_clust_{cl}.png"
# clust_fmt = out_fmt_clust + "_clusters.npy"
# mlab_fmt = out_fmt_clust + "_dict_index_mlab.yaml"

# Classif
bmg224_dir = '../../..'
dict_date_pdfn = {
    '2022_12_16': [
        bmg224_dir 
        + '/manuscripts/mgefish/data/HiPRFISH_probe_design' 
        + '/welch2016_5b_no_633_channel.csv',
        '5bit_no633'
    ],
    '2023_02_08': [
        bmg224_dir 
        + '/manuscripts/mgefish/data/HiPRFISH_probe_design' 
        + '/welch2016_5b_no_633_channel.csv',
        '5bit_no633'
    ],
    '2023_02_18': [
        bmg224_dir 
        + '/manuscripts/mgefish/data/HiPRFISH_probe_design'
        + '/welch2016_5b_no_633_channel.csv',
        '5bit_no633'
    ],
    '2023_10_16': [
        bmg224_dir 
        + '/harvard_dental/pick_distant_barcodes/2023_08_01_order'
        + '/welch2016_7b_distant_v2.csv',
        '7bit_no405'
    ],
    '2023_10_18': [
        bmg224_dir 
        + '/harvard_dental/pick_distant_barcodes/2023_08_01_order'
        + '/welch2016_7b_distant_v2.csv',
        '7bit_no405'
    ],
    '2024_03_02': [
        bmg224_dir 
        + '/harvard_dental/pick_distant_barcodes/2023_12_22_order'
        + '/welch2016_5b_distant_v4_doubleflank.csv',
        '5bit_no633'
    ],
    '2024_04_16': [
        bmg224_dir 
        + '/harvard_dental/pick_distant_barcodes/2023_12_22_order'
        + '/welch2016_5b_distant_v4_doubleflank.csv',
        '5bit_no633'
    ],
    '2024_04_19': [
        bmg224_dir 
        + '/harvard_dental/pick_distant_barcodes/2023_12_22_order'
        + '/welch2016_5b_distant_v4_doubleflank.csv',
        '5bit_no633'
    ],
    '2024_04_24': [
        bmg224_dir 
        + '/harvard_dental/pick_distant_barcodes/2023_12_22_order'
        + '/welch2016_5b_distant_v4_doubleflank.csv',
        '5bit_no633'
    ],
    '2024_04_27': [
        bmg224_dir 
        + '/harvard_dental/pick_distant_barcodes/2023_12_22_order'
        + '/welch2016_5b_distant_v4_doubleflank.csv',
        '5bit_no633'
    ],
    '2024_05_03': [
        bmg224_dir 
        + '/harvard_dental/pick_distant_barcodes/2023_12_22_order'
        + '/welch2016_5b_distant_v4_doubleflank.csv',
        '5bit_no633'
    ],
}
out_fmt_classif = out_dir + "/classif"
classif_fmt = out_fmt_classif + "/{date}_{sn}_M_{m}_cell_scinames.npy"
classif_done_fmt = out_fmt_classif + "/classif_done.txt"
# classif_fmt = out_fmt_classif + "/{date}_{sn}_dict_cluster_barcode.yaml"
# spec_classif_fmt = out_fmt_classif + "/spectra_plots/{date}_{sn}_spec_clust_{cl}.png"

# plot spectra
plot_spectra_fmt = out_fmt_classif + '/spectra/{date}_{sn}_sciname_{sci}.pdf'
plot_spectra_done_fmt = out_fmt_classif + '/spectra/plot_spectra_done.txt'

# get coords
out_dir_coords = out_fmt_classif + "/coords_240705_cosdist"
centroid_sciname_fmt = out_dir_coords + '/{date}_{sn}_centroid_sciname.csv'
# out_dir_plot_classif = out_dir_plot + '/color_240416'
# classif_plot_fmt = out_dir_plot_classif + '/{date}_{sn}_M_{m}_classif.png'
# classif_plot_all_fmt = sample_compare_dir + '/alltiles/color_240416/{date}_{sn}_classif_alltiles_white.png'

# absolute_abundances
out_fmt_absabund = out_fmt_classif + '/absolute_abundance'
volume_agg_fmt = out_fmt_absabund + '/{date}_{sn}_absolute_abundance_volume_aggregated.npy'
scinames_agg_fmt = out_fmt_absabund + '/{date}_{sn}_absolute_abundance_scinames_aggregated.npy'

# recolor
sciname_list = [
    'Pasteurellaceae',
    'Corynebacterium',
    'Veillonella',
    'Actinomyces',
    'Selenomonas',
    'Rothia',
    'Porphyromonas',
    'Capnocytophaga',
    'Prevotella',
    'Streptococcus',
    'Gemella',
    'Campylobacter',
    'Lautropia',
    'Leptotrichia',
    'Neisseriaceae',
    'Treponema',
    'Fusobacterium',
    'TM7'
]
cmap = plt.get_cmap('gist_rainbow')
colors = [cmap(i)[:3] for i in np.linspace(0,1,len(sciname_list))]
colors[13] = [0.75,0.75,0.75]
colors[1] = np.array([176,2,104])/255
colors[8] = np.array([184,155,69])/255
colors[6] = np.array([245,164,159])/255
colors[5] = np.array([221,159,239])/255
colors[14] = np.array([54,148,161])/255
colors.insert(17, colors.pop(1))
colors.insert(4, colors.pop(6))
colors.insert(9, colors.pop(12))
colors.insert(5, colors.pop(8))
# colors = [c + (1,) for c in colors]
dict_sciname_color = dict(zip(sciname_list, colors))
dict_sciname_color['Neisseria'] = dict_sciname_color['Neisseriaceae']
dict_sciname_color['Saccharibacteria'] = dict_sciname_color['TM7']

clip_dict = {
}
out_dir_recolor = out_dir_plot + '/recolor_240705_cosdist'
plot_recolor_fmt = out_dir_recolor + '/{cscale}/{date}_{sn}_M_{m}_classif_cscale_{cscale}.png'

sample_compare_dir = config['output_dir'] + '/compare_samples'
plot_recolor_all_fmt = sample_compare_dir + '/alltiles/recolor_240705_cosdist/cscale_{cscale}/{date}_{sn}_classif_alltiles_cscale_{cscale}.png'
recolor_done_fmt = out_dir_recolor + '/snakemake_recolor_done.txt'

# color_legend
color_legend_fn = sample_compare_dir + '/alltiles/recolor_240418_04/taxon_legend.pdf'

# rgb_tile
plot_rgb_tile_fmt = sample_compare_dir + '/rgbs/{date}_{sn}_alltiles_rgb.png'
rgb_tile_done_fmt = out_dir + '/snakemake_rgb_tile_done.txt'

# get_abundances
abundances_fmt = out_fmt_classif + '/{date}_{sn}_scinames.npy'

# # global_autocorr
# spatial_dir = out_dir + '/spatial_statistics'
# moran_dir = spatial_dir + '/morans_i'
# adjacency_hist_fmt = moran_dir + '/{date}_{sn}_adjacency_histogram.png'
# moran_plot_fmt = moran_dir + "/{date}_{sn}_sciname_{scn}_moran_plot.png"
# scatter_plot_fmt = moran_dir + "/{date}_{sn}_sciname_{scn}_scatter_plot.png"
# moran_table_fmt = moran_dir + "/{date}_{sn}_moran_values.csv"
# moran_bv_dir = spatial_dir + '/morans_bv'
# scatter_plot_bv_fmt = moran_bv_dir + "/{date}_{sn}_scinames_{scn0}_{scn1}_scatter_plot.png"
# moran_plot_bv_fmt = moran_bv_dir + "/{date}_{sn}_scinames_{scn0}_{scn1}_moran_plot.png"
# moran_bv_table_fmt = moran_bv_dir + "/{date}_{sn}_moran_bv_values.csv"


# =============================================================================
# Rule all output
# =============================================================================

segs_done = get_fns(segs_done_fmt)
classif_done = get_fns(classif_done_fmt)
plot_spectra_done = get_fns(plot_spectra_done_fmt)
coords_done = get_fns(centroid_sciname_fmt)
recolor_done = get_fns(recolor_done_fmt)
rgb_tile_done = get_fns(rgb_tile_done_fmt)
abundances_done = get_fns(abundances_fmt)
scinames_agg_done = get_fns(scinames_agg_fmt)
# global_autocorr_done = get_fns(moran_bv_table_fmt)

# seg_fns_all = get_fns(seg_fmt)
# props_fns_all = get_fns(props_fmt)
# plot_fns_all = get_fns(plot_fmt)
# rgb_fns_all = get_fns(rgb_fmt)


# =============================================================================
# Snake rules
# =============================================================================

rule all:
    input:
        plot_spectra_done,
        coords_done,
        recolor_done,
        scinames_agg_done
        # abundances_done,
        


rule segment:
    input:
        lambda wildcards: get_czi_fns(f'{wildcards.date}',f'{wildcards.sn}')
    output:
        segs_done_fn = segs_done_fmt,
    run:
        # Get number of tiles or scenes
        M, mtype = fsi.get_ntiles(input[0])
        # Get the resolutions
        resolutions = [fsi.get_resolution(fn) for fn in input]
        # Get the lasers
        lasers = [fsi.get_laser(fn) for fn in input]
        # remove 405 channel
        czi_fns = [fn for fn, l in zip(input, lasers) if l != 405]
        resolutions = [r for r, l in zip(resolutions, lasers) if l != 405]
        lasers = [l for l in lasers if l != 405]
        czi_fns = [x for _, x in sorted(zip(lasers, czi_fns))]
        resolutions = [x for _, x in sorted(zip(lasers, resolutions))]
        lasers = sorted(lasers)
        # Get shifts
        shifts = []
        for m in range(M):
            raws = [fsi.load_raw(fn, m, mtype) for fn in czi_fns]
            raws = [fsi.reshape_aics_image(r) for r in raws]
            # some images have different pixel resolution, correct that
            raws = fsi.match_resolutions_and_size(raws, resolutions)
            image_max_norm = [fsi.max_norm(r) for r in raws]
            sh = fsi._get_shift_vectors(image_max_norm)
            # print(sh)
            shifts.append(sh)
        # Some of the shifts are clearly wrong, fix those
        sh_arr = np.array(shifts)
        # for k in range(1, len(lasers)):
        #     sh_i = sh_arr[:, k, :]
        #     # print("Shifts", lasers[k], ":")
        #     # print(sh_i)
        #     # address large deviatinos from typical
        #     sh_arr[:, k, :] = fsi.replace_outlier_shifts(sh_i)
        # Now shift the raw images
        for m in range(M):
            rgb_fn = rgb_fmt.format(date=wildcards.date, sn=wildcards.sn, m=m)
            raws = [fsi.load_raw(fn, m, mtype) for fn in czi_fns]
            raws = [fsi.reshape_aics_image(r) for r in raws]
            # some images have different pixel resolution, correct that
            raws = fsi.match_resolutions_and_size(raws, resolutions)
            raws_shift = fsi._shift_images(
                raws, sh_arr[m, :, :], max_shift=config['max_shift']
            )
            stack = np.dstack(raws_shift)
            stack_sum = np.sum(stack, axis=2)
            pre = sf.pre_process(
                stack_sum, 
                gauss=config['gauss'], 
                diff_gauss=eval(config['diff_gauss'])
                )
            mask = sf.get_background_mask(
                stack_sum,
                bg_smoothing=config['bg_smoothing'],
                n_clust_bg=config['n_clust_bg'],
                top_n_clust_bg=config['top_n_clust_bg'],
            )
            seg = sf.segment(pre, mask)
            props = sf.measure_regionprops(seg, stack_sum)
            spec = fsi.get_cell_average_spectra(seg, stack)
            props = props.merge(
                pd.DataFrame(spec), left_index=True, right_index=True
            )
            # ncells += props.shape[0]

            seg_fn = seg_fmt.format(date=wildcards.date, sn=wildcards.sn, m=m)
            props_fn = props_fmt.format(date=wildcards.date, sn=wildcards.sn, m=m)
            plot_fn = plot_fmt.format(date=wildcards.date, sn=wildcards.sn, m=m)
            rgb_fn = rgb_fmt.format(date=wildcards.date, sn=wildcards.sn, m=m)

            for f in [seg_fn, props_fn, plot_fn, rgb_fn]:
                odir = os.path.split(f)[0]
                if not os.path.exists(odir):
                    os.makedirs(odir)
                    print("Made dir:", odir)

            np.save(seg_fn, seg)
            print("Wrote:", seg_fn)
            props.to_csv(props_fn, index=False)
            print("Wrote:", props_fn)
            
            # ip.plot_image(stack_sum, cmap="inferno", im_inches=10)
            fig, ax, _ = ip.plot_image(ip.seg2rgb(seg), im_inches=config['im_inches'])
            plt.figure(fig)
            ip.save_fig(plot_fn, dpi=config['dpi'], bbox_inches=0)
            plt.close()
            print("Wrote:", plot_fn)

            rgb = np.dstack([fsi.max_norm(r, type='sum') for r in raws_shift])
            rgb = rgb[:,:,:3]
            fig, ax, _ = ip.plot_image(rgb, im_inches=config['im_inches'])
            plt.figure(fig)
            ip.save_fig(rgb_fn, dpi=config['dpi'], bbox_inches=0)
            plt.close()
            print("Wrote:", rgb_fn)
        
        seg_fn = seg_fmt.format(date=wildcards.date, sn=wildcards.sn, m=0)
        if os.path.exists(seg_fn):
            with open(output.segs_done_fn, 'w') as f:
                f.write('snakemake seg rule file')


        # seg_fns = lambda wildcards: get_seg_fns(
        #     f'{wildcards.date}', f'{wildcards.sn}', seg_fmt
        # ),
        # props_fns = lambda wildcards: get_seg_fns(
        #     f'{wildcards.date}', f'{wildcards.sn}', props_fmt
        # ),
        # plot_fns = lambda wildcards: get_seg_fns(
        #     f'{wildcards.date}', f'{wildcards.sn}', plot_fmt
        # ),
        # rgb_fns = lambda wildcards: get_seg_fns(
        #     f'{wildcards.date}', f'{wildcards.sn}', rgb_fmt
        # ),




rule classif_cosdist:
    input:
        czi_fns = lambda wildcards: get_czi_fns(f'{wildcards.date}',f'{wildcards.sn}'),
        segs_done_fn = segs_done_fmt,
    output:
        classif_done_fn = classif_done_fmt
    run:
        # Load reference spectra
        pdfn, bc_type = dict_date_pdfn[wildcards.date]
        probe_design = pd.read_csv(pdfn)
        barcodes = probe_design['code']
        sci_names = probe_design['sci_name']
        dict_bc_sciname = dict(zip(barcodes, sci_names))
        barcodes = np.unique(barcodes)
        sci_names = [dict_bc_sciname[bc] for bc in barcodes]
        ref_spec = fhc.get_reference_spectra(
            barcodes, bc_type, config['ref_dir']
            )
        ref_spec_med = [np.median(r, axis=0) for r in ref_spec]
        ref_spec_arr = np.vstack(ref_spec_med)

        # Classify each tile
        # Gather spectra to get background
        M, mtype = fsi.get_ntiles(input.czi_fns[0])
        spec_all = []
        for m in range(M):
            # Load props
            props_fn = props_fmt.format(date=wildcards.date, sn=wildcards.sn, m=m)
            p = pd.read_csv(props_fn)
            if p.shape[0] > 0:
                # construct spectra array
                n_chan_spec = fhc.get_n_spec_channels(bc_type)
                cols_spec = np.arange(n_chan_spec).astype(str).tolist()
                spec_arr = p[cols_spec].values
                spec_all.append(spec_arr)

        spec_all_arr = np.vstack(spec_all)
        # Get background
        bg_med = np.median(spec_all_arr, axis=0)

        for m in range(M):
            # Load props
            props_fn = props_fmt.format(date=wildcards.date, sn=wildcards.sn, m=m)
            p = pd.read_csv(props_fn)
            # construct spectra array
            n_chan_spec = fhc.get_n_spec_channels(bc_type)
            cols_spec = np.arange(n_chan_spec).astype(str).tolist()
            spec_arr = p[cols_spec].values
            # if p.shape[0] > 0:

            # Scale background to sum of each cell spectrum
            row_sums = np.sum(spec_arr, axis=1)
            bg_med_norm = bg_med / np.sum(bg_med)
            # Subtract bg from spectra
            bg_mat = np.matmul(row_sums[:,None], bg_med_norm[None,:])
            spec_arr_bgsub = spec_arr - bg_mat
            # Zero out negatives
            spec_arr_bgsub_negzero = spec_arr_bgsub.copy()
            spec_arr_bgsub_negzero[spec_arr_bgsub_negzero < 0] = 0

            # Get distances to ref spectra
            cl_dist_matrix = cdist(
                    spec_arr_bgsub_negzero, 
                    ref_spec_arr, 
                    metric=fhc.channel_cosine_intensity_allonev2
                )            
            # Get indices of min distances
            rs_ind = np.argmin(cl_dist_matrix, axis=1)
            # Get sciname for each cell
            scinames_cosdist = np.array([sci_names[i] for i in rs_ind])
            # else:
            #     scinames_cosdist = np.array([])

            # Save 
            classif_fn = classif_fmt.format(
                date=wildcards.date, sn=wildcards.sn, m=m
            )
            odir = os.path.split(classif_fn)[0]
            if not os.path.exists(odir):
                os.makedirs(odir)
                print("Made dir:", odir)
            np.save(classif_fn, scinames_cosdist)

        with open(output.classif_done_fn, 'w') as f:
            f.write('Snakemake classif rule done')

# rule classif_matmul:
#     input:
#         czi_fns = lambda wildcards: get_czi_fns(f'{wildcards.date}',f'{wildcards.sn}'),
#         segs_done_fn = segs_done_fmt,
#     output:
#         classif_done_fn = classif_done_fmt
#     run:
#         # Load reference spectra
#         pdfn, bc_type = dict_date_pdfn[wildcards.date]
#         probe_design = pd.read_csv(pdfn)
#         barcodes = probe_design['code']
#         sci_names = probe_design['sci_name']
#         dict_bc_sciname = dict(zip(barcodes, sci_names))
#         barcodes = np.unique(barcodes)
#         sci_names = [dict_bc_sciname[bc] for bc in barcodes]
#         ref_spec = fhc.get_reference_spectra(
#             barcodes, bc_type, config['ref_dir']
#             )
#         ref_spec_med = [np.median(r, axis=0) for r in ref_spec]
#         ref_spec_arr = np.vstack(ref_spec_med)
#         ref_spec_arr_sumnorm = ref_spec_arr / np.sum(ref_spec_arr, axis=1)[:,None]

#         # Classify each tile
#         # Gather spectra to get background
#         M, mtype = fsi.get_ntiles(input.czi_fns[0])
#         spec_all = []
#         for m in range(M):
#             # Load props
#             props_fn = props_fmt.format(date=wildcards.date, sn=wildcards.sn, m=m)
#             p = pd.read_csv(props_fn)
#             if p.shape[0] > 0:
#                 # construct spectra array
#                 n_chan_spec = fhc.get_n_spec_channels(bc_type)
#                 cols_spec = np.arange(n_chan_spec).astype(str).tolist()
#                 spec_arr = p[cols_spec].values
#                 spec_all.append(spec_arr)

#         spec_all_arr = np.vstack(spec_all)
#         # Get background
#         bg_med = np.median(spec_all_arr, axis=0)

#         for m in range(M):
#             # Load props
#             props_fn = props_fmt.format(date=wildcards.date, sn=wildcards.sn, m=m)
#             p = pd.read_csv(props_fn)
#             # construct spectra array
#             n_chan_spec = fhc.get_n_spec_channels(bc_type)
#             cols_spec = np.arange(n_chan_spec).astype(str).tolist()
#             spec_arr = p[cols_spec].values
#             # if p.shape[0] > 0:

#             # Scale background to sum of each cell spectrum
#             row_sums = np.sum(spec_arr, axis=1)
#             bg_med_norm = bg_med / np.sum(bg_med)
#             # Subtract bg from spectra
#             bg_mat = np.matmul(row_sums[:,None], bg_med_norm[None,:])
#             spec_arr_bgsub = spec_arr - bg_mat
#             # Zero out negatives
#             spec_arr_bgsub_negzero = spec_arr_bgsub.copy()
#             spec_arr_bgsub_negzero[spec_arr_bgsub_negzero < 0] = 0

#             # Multiply spec by reference
#             mat_mul = np.matmul(spec_arr_bgsub_negzero, ref_spec_arr.T)

#             # # Classify each spectrum
#             # nn_dists, nn_inds = nn_ref.kneighbors(cl_dist_matrix)
#             # rs_ind = np.argmax(mat_mul, axis=1)
#             # bc = barcodes[rs_ind]
#             # sciname = sci_names[rs_ind]
#             # dict_cl_bc[cl] = sciname
       
#             # Get indices of min distances
#             rs_ind = np.argmax(mat_mul, axis=1)
#             # Get sciname for each cell
#             scinames_matmul = np.array([sci_names[i] for i in rs_ind])
#             # else:
#             #     scinames_cosdist = np.array([])

#             # Save 
#             classif_fn = classif_fmt.format(
#                 date=wildcards.date, sn=wildcards.sn, m=m
#             )
#             odir = os.path.split(classif_fn)[0]
#             if not os.path.exists(odir):
#                 os.makedirs(odir)
#                 print("Made dir:", odir)
#             np.save(classif_fn, scinames_matmul)

#         with open(output.classif_done_fn, 'w') as f:
#             f.write('Snakemake classif rule done')


rule plot_spectra:
    input:
        czi_fns = lambda wildcards: get_czi_fns(f'{wildcards.date}',f'{wildcards.sn}'),
        segs_done_fn = segs_done_fmt,
        classif_done_fn = classif_done_fmt,
    output:
        plot_spectra_done_fn = plot_spectra_done_fmt
    run:
        # Load reference spectra
        pdfn, bc_type = dict_date_pdfn[wildcards.date]
        probe_design = pd.read_csv(pdfn)
        barcodes = probe_design['code']
        sci_names = probe_design['sci_name']
        dict_bc_sciname = dict(zip(barcodes, sci_names))
        barcodes = np.unique(barcodes)
        sci_names = [dict_bc_sciname[bc] for bc in barcodes]
        ref_spec = fhc.get_reference_spectra(
            barcodes, bc_type, config['ref_dir']
            )
        ref_spec_med = [np.median(r, axis=0) for r in ref_spec]
        ref_spec_arr = np.vstack(ref_spec_med)
        
        # n tiles
        M, mtype = fsi.get_ntiles(input.czi_fns[0])

        # Get background curve
        spec_all = []
        for m in range(M):
            # Load props
            props_fn = props_fmt.format(date=wildcards.date, sn=wildcards.sn, m=m)
            props = pd.read_csv(props_fn)
            # construct spectra array
            n_chan_spec = fhc.get_n_spec_channels(bc_type)
            cols_spec = np.arange(n_chan_spec).astype(str).tolist()
            spec_arr = props[cols_spec].values
            spec_all.append(spec_arr)

        spec_all_arr = np.vstack(spec_all)
        # Get background
        bg_med = np.median(spec_all_arr, axis=0)

        # Accumulate spectra for each sciname 
        dict_sciname_spec = defaultdict(list)
        for m in range(M):
            # load props 
            props_fn = props_fmt.format(
                date=wildcards.date, sn=wildcards.sn, m=m
            )
            props = pd.read_csv(props_fn)

            # Load scinames
            classif_fn = classif_fmt.format(
                date=wildcards.date, sn=wildcards.sn, m=m
            )
            scinames_cosdist = np.load(classif_fn)

            # construct spectra array
            n_chan_spec = fhc.get_n_spec_channels(bc_type)
            cols_spec = np.arange(n_chan_spec).astype(str).tolist()
            spec_arr = props[cols_spec].values

            # Scale background to sum of each cell spectrum
            row_sums = np.sum(spec_arr, axis=1)
            bg_med_norm = bg_med / np.sum(bg_med)
            # Subtract bg from spectra
            bg_mat = np.matmul(row_sums[:,None], bg_med_norm[None,:])
            spec_arr_bgsub = spec_arr - bg_mat
            # Zero out negatives
            spec_arr_bgsub_negzero = spec_arr_bgsub.copy()
            spec_arr_bgsub_negzero[spec_arr_bgsub_negzero < 0] = 0
            # Add spectra to dict
            for c in np.unique(scinames_cosdist):
                # print('Cluster:', c)
                bool_c = scinames_cosdist == c
                spec_sub = spec_arr_bgsub_negzero[bool_c,:]

                dict_sciname_spec[c].append(spec_sub)
        
        # Plot spectra
        for c, spec_list in dict_sciname_spec.items():
            spec_sub = np.vstack(spec_list)
            spec_sub_sumnorm = spec_sub / np.sum(spec_sub, axis=1)[:,None]

            fig, ax = ip.general_plot(dims=eval(config['spec_dims']), col='w')
            color = dict_sciname_color[c]
            # fsi.plot_cell_spectra(ax, spec_sub, {'lw':2,'alpha':0.2,'color':'r'})
            fsi.plot_cell_spectra(
                ax, spec_sub_sumnorm, {'lw':2,'alpha':0.05,'color':color}
            )

            rs_ind = np.where(np.array(sci_names) == c)[0][0]
            rs = ref_spec_med[rs_ind][None,:]
            rs = rs / np.sum(rs)
            fsi.plot_cell_spectra(ax, rs, {'lw':2,'alpha':1,'color':'w'})

            ylim = ax.get_ylim()

            xs = np.array(config['plot_spectra']['xs'])
            xs = xs[xs < spec_sub.shape[1]]
            for x in xs:
                ax.plot([x,x], ylim, color=(0.5,0.5,0.5), lw=0.5)
            
            bc = barcodes[rs_ind]
            ax.set_title(c + ': ' + str(bc), color='w')

            plot_spectra_fn = plot_spectra_fmt.format(
                date=wildcards.date, sn=wildcards.sn, sci=c
            )               
            ip.save_fig(plot_spectra_fn)
            
        with open(output.plot_spectra_done_fn, 'w') as f:
            f.write('snakemake plot_spectra rule done')         




rule get_coords:
    input:
        czi_fns = lambda wildcards: get_czi_fns(f'{wildcards.date}',f'{wildcards.sn}'),
        classif_done_fn = classif_done_fmt
    output:
        coords_fn = centroid_sciname_fmt
    run:
        czi_fns = input.czi_fns
        # # print(czi_fns)
        # # Get bc sciname dict
        # pdfn, bc_type = dict_date_pdfn[wildcards.date]
        # probe_design = pd.read_csv(pdfn)
        # barcodes = probe_design['code']
        # sci_names = probe_design['sci_name']
        # dict_bc_sciname = dict(zip(barcodes, sci_names))
        # filenames
        # mlab_fn = mlab_fmt.format(
        #     date=wildcards.date, sn=wildcards.sn
        # )
        # clust_fn = clust_fmt.format(
        #     date=wildcards.date, sn=wildcards.sn
        # )
        # classif_fn = classif_fmt.format(
        #     date=wildcards.date, sn=wildcards.sn
        # )

        # # props
        # prop_glob = props_fmt.format(
        #     date=wildcards.date, sn=wildcards.sn, m='*'
        # )
        # prop_fns = glob.glob(prop_glob)
        # Ms = [int(re.search('(?<=_M_)\d+',fn)[0]) for fn in prop_fns]
        # prop_fns = [x for _, x in sorted(zip(Ms, prop_fns))]

        # # registered images
        # reg_glob = reg_fmt.format(
        #     date=wildcards.date, sn=wildcards.sn, m='*'
        # )
        # reg_fns = glob.glob(reg_glob)
        # Ms = [int(re.search('(?<=_registered_)\d+',fn)[0]) for fn in reg_fns]
        # reg_fns = [x for _, x in sorted(zip(Ms, reg_fns))]

        # # segs
        seg_glob = seg_fmt.format(date=wildcards.date, sn=wildcards.sn, m='*')
        # seg_fns = glob.glob(seg_glob)
        # Ms = [int(re.search('(?<=_M_)\d+',fn)[0]) for fn in seg_fns]
        # seg_fns = [x for _, x in sorted(zip(Ms, seg_fns))]

        # # Load mlab dict
        # with open(mlab_fn, 'r') as f:
        #     dict_m_lab_ind = yaml.unsafe_load(f)
        # # Load clusters
        # clust_agg = np.load(clust_fn)
        # cl_unq = np.unique(clust_agg)
        # # Load classif
        # with open(classif_fn, 'r') as f:
        #     dict_cl_bc = yaml.unsafe_load(f)

        # Get upper left corners for each tile
        M, mtype = fsi.get_ntiles(czi_fns[0])
        res_umpix = fsi.get_resolution(czi_fns[0]) * 1e6
        if mtype == 'M':
            ncols = int(fsi.get_metadata_value(czi_fns[0], 'TilesX')[0])
            nrows = int(fsi.get_metadata_value(czi_fns[0], 'TilesY')[0])
            overl = float(fsi.get_metadata_value(
                czi_fns[0], 'TileAcquisitionOverlap'
            )[0])
            cols = np.tile(np.arange(ncols), nrows)
            rows = np.repeat(np.arange(nrows), ncols)
            seg_fn = seg_fmt.format(
                date=wildcards.date, sn=wildcards.sn, m=0
            )
            seg = np.load(seg_fn)
            shp = np.array(seg.shape[:2])
            im_r, im_c = shp - shp * overl
            ul_corners = []
            for m in range(M):
                c, r = cols[m], rows[m]
                ulc = np.array([r * im_r, c * im_c])
                ul_corners.append(ulc)

            # # Plot classified tiled image
            # tile_shp = (
            #     (nrows - 1)*int(math.ceil(im_r)) + shp[0], 
            #     (ncols - 1)*int(math.ceil(im_c)) + shp[1]
            # )
            # classif_tile = np.zeros((
            #     tile_shp[0], tile_shp[1],len(colors[0])
            # ))
            # sum_tile = np.zeros((tile_shp[0], tile_shp[1]))

            # Get cell absolute locations
            # dict_ind_centroid_sciname = {}
            ul_corners = np.array(ul_corners)
            r_lims = np.unique(ul_corners[:,0])
            c_lims = np.unique(ul_corners[:,1])
            df_centroid_sciname = pd.DataFrame([])
            for m in range(M):
                # Get image corner values
                c, r = cols[m], rows[m]
                ulc = ul_corners[m]
                # Get limits to remove cells from overlap
                r_lim, c_lim = [1e15]*2
                if r < np.max(rows):
                    r_lim = r_lims[r+1]
                if c < np.max(cols):
                    c_lim = c_lims[c+1]

                # Adjust cell locations 
                props_fn = props_fmt.format(
                    date=wildcards.date, sn=wildcards.sn, m=m
                )
                prop = pd.read_csv(props_fn)
                if prop.shape[0] > 0:
                    # Centroid
                    centroids = np.array([eval(c) for c in prop['centroid']])
                    centroids += ulc
                    bool_clim = centroids[:,1] < c_lim
                    bool_rlim = centroids[:,0] < r_lim
                    prop['centroid_adj'] = centroids.tolist()
                    # # Bbox
                    # bboxes = np.array([eval(b) for b in prop['bbox']])
                    # bboxes += np.tile(ulc, 2).astype(int)
                    # prop['bbox_adj'] = bboxes.tolist()

                    # Add scinames
                    classif_fn = classif_fmt.format(
                        date=wildcards.date, sn=wildcards.sn, m=m
                    )
                    scinames_cosdist = np.load(classif_fn)
                    prop['sciname'] = scinames_cosdist
                    # Add tile
                    prop['tile'] = m
                    # Filter based on location
                    prop_filt = prop[bool_rlim*bool_clim]
                    # subset props
                    prop_sub = prop_filt[[
                        'centroid_adj','sciname','tile','label',
                        ]]
                    # append
                    df_centroid_sciname = df_centroid_sciname.append(prop_sub)
            
                
        else:
            dict_ind_centroid_sciname = {}
            for m in range(M):
                props_fn = props_fmt.format(
                    date=wildcards.date, sn=wildcards.sn, m=m
                )
                prop = pd.read_csv(props_fn)

                # get scinames
                classif_fn = classif_fmt.format(
                    date=wildcards.date, sn=wildcards.sn, m=m
                )
                scinames_cosdist = np.load(classif_fn)
                prop['sciname'] = scinames_cosdist
                # Add tile
                prop['tile'] = m
                df_centroid_sciname = prop[[
                    'centroid','sciname','tile','label',
                    ]]
        
        # centroid_sciname_fn = centroid_sciname_fmt.format(
        #     date=wildcards.date, sn=wildcards.sn
        # )
        df_centroid_sciname.columns = ['coord', 'sciname','tile','label']
        df_centroid_sciname.to_csv(output.coords_fn, index=False)

rule absolute_abundances:
    input:
        czi_fns = lambda wildcards: get_czi_fns(f'{wildcards.date}',f'{wildcards.sn}'),
        classif_done_fn = classif_done_fmt
    output:
        volume_agg_fn = volume_agg_fmt,
        scinames_agg_fn = scinames_agg_fmt,
    run:
        czi_fns = input.czi_fns

        # Get number of tiles or scenes
        M, mtype = fsi.get_ntiles(input.czi_fns[0])
        # Get the resolutions
        resolutions = [fsi.get_resolution(fn) for fn in input.czi_fns]
        # Get the lasers
        lasers = [fsi.get_laser(fn) for fn in input.czi_fns]
        # remove 405 channel
        czi_fns = [fn for fn, l in zip(input.czi_fns, lasers) if l != 405]
        resolutions = [r for r, l in zip(resolutions, lasers) if l != 405]
        lasers = [l for l in lasers if l != 405]
        czi_fns = [x for _, x in sorted(zip(lasers, czi_fns))]
        resolutions = [x for _, x in sorted(zip(lasers, resolutions))]
        lasers = sorted(lasers)

        # Get resolutions
        resolution_xy = fsi.get_resolution(input.czi_fns[0])
        res_xy_umpix = resolution_xy * 1e6

        resolution_z = fsi.get_resolution(input.czi_fns[0], dim='Z')
        res_z_umpix = resolution_z * 1e6

        voxel_um3pix = res_xy_umpix**2 * res_z_umpix

        # Get upper left corners for each tile
        M, mtype = fsi.get_ntiles(czi_fns[0])
        res_umpix = fsi.get_resolution(czi_fns[0]) * 1e6
        if mtype == 'M':
            ncols = int(fsi.get_metadata_value(czi_fns[0], 'TilesX')[0])
            nrows = int(fsi.get_metadata_value(czi_fns[0], 'TilesY')[0])
            overl = float(fsi.get_metadata_value(
                czi_fns[0], 'TileAcquisitionOverlap'
            )[0])
            cols = np.tile(np.arange(ncols), nrows)
            rows = np.repeat(np.arange(nrows), ncols)
            seg_fn = seg_fmt.format(
                date=wildcards.date, sn=wildcards.sn, m=0
            )
            seg = np.load(seg_fn)
            shp = np.array(seg.shape[:2])
            im_r, im_c = shp - shp * overl
            ul_corners = []
            for m in range(M):
                c, r = cols[m], rows[m]
                ulc = np.array([r * im_r, c * im_c])
                ul_corners.append(ulc)

            # Get cell counts and area but remove overlap
            ul_corners = np.array(ul_corners)
            r_lims = np.unique(ul_corners[:,0])
            c_lims = np.unique(ul_corners[:,1])
            df_centroid_sciname = pd.DataFrame([])
            volume_agg = 0
            scinames_agg = []
            for m in range(M):
                # Get image corner values
                c, r = cols[m], rows[m]
                ulc = ul_corners[m]
                # Get limits to remove cells from overlap
                r_lim, c_lim = [1e15]*2
                if r < np.max(rows):
                    r_lim = r_lims[r+1]
                if c < np.max(cols):
                    c_lim = c_lims[c+1]

                
                # Remove overlap cells
                props_fn = props_fmt.format(
                    date=wildcards.date, sn=wildcards.sn, m=m
                )
                prop = pd.read_csv(props_fn)
                if prop.shape[0] > 0:
                    # Centroid
                    centroids = np.array([eval(c) for c in prop['centroid']])
                    centroids += ulc
                    bool_clim = centroids[:,1] < c_lim
                    bool_rlim = centroids[:,0] < r_lim
                    prop['centroid_adj'] = centroids.tolist()
                    # Add scinames
                    classif_fn = classif_fmt.format(
                        date=wildcards.date, sn=wildcards.sn, m=m
                    )
                    scinames_cosdist = np.load(classif_fn)
                    prop['sciname'] = scinames_cosdist
                    # Filter based on location
                    prop_filt = prop[bool_rlim*bool_clim]
                    # Add to total list
                    scinames_agg += prop_filt['sciname'].tolist()

                    # Get the mask
                    try:
                        raws = [fsi.load_raw(fn, m, mtype) for fn in czi_fns]
                        raws = [fsi.reshape_aics_image(r) for r in raws]
                    except:
                        raws = [np.zeros_like(r) for r in raws]
                    if np.max([np.max(r) for r in raws]) > 0:
                        raws = fsi.match_resolutions_and_size(raws, resolutions)
                        image_max_norm = [fsi.max_norm(r) for r in raws]
                        sh = fsi._get_shift_vectors(image_max_norm)
                        raws_shift = fsi._shift_images(
                            raws, sh, max_shift=config['max_shift']
                        )
                        stack = np.dstack(raws_shift)
                        stack_sum = np.sum(stack, axis=2)
                        mask = sf.get_background_mask(
                            stack_sum,
                            bg_smoothing=config['bg_smoothing'],
                            n_clust_bg=config['n_clust_bg'],
                            top_n_clust_bg=config['top_n_clust_bg'],
                        )
                        # Get volume measured
                        n_pix = np.sum(mask > 0)
                        vol = n_pix * voxel_um3pix
                        volume_agg += vol
     
        else:
            scinames_agg = []
            volume_agg = 0
            for m in range(M):
                # get scinames
                classif_fn = classif_fmt.format(
                    date=wildcards.date, sn=wildcards.sn, m=m
                )
                scinames_cosdist = np.load(classif_fn)
                scinames_agg += scinames_cosdist.tolist()

                # Get the mask
                try:
                    raws = [fsi.load_raw(fn, m, mtype) for fn in czi_fns]
                    raws = [fsi.reshape_aics_image(r) for r in raws]
                except:
                    raws = [np.zeros_like(r) for r in raws]
                if np.max([np.max(r) for r in raws]) > 0:
                    raws = fsi.match_resolutions_and_size(raws, resolutions)
                    image_max_norm = [fsi.max_norm(r) for r in raws]
                    sh = fsi._get_shift_vectors(image_max_norm)
                    raws_shift = fsi._shift_images(
                        raws, sh, max_shift=config['max_shift']
                    )
                    stack = np.dstack(raws_shift)
                    stack_sum = np.sum(stack, axis=2)
                    mask = sf.get_background_mask(
                        stack_sum,
                        bg_smoothing=config['bg_smoothing'],
                        n_clust_bg=config['n_clust_bg'],
                        top_n_clust_bg=config['top_n_clust_bg'],
                    )
                    # Get volume measured
                    n_pix = np.sum(mask > 0)
                    vol = n_pix * voxel_um3pix
                    volume_agg += vol
        
        # Save values
        volume_agg = np.array([volume_agg])
        np.save(output.volume_agg_fn, volume_agg)

        scinames_agg = np.array(scinames_agg)
        np.save(output.scinames_agg_fn, scinames_agg)



rule recolor:
    input:
        czi_fns = lambda wildcards: get_czi_fns(f'{wildcards.date}',f'{wildcards.sn}'),
        classif_done_fn = classif_done_fmt
    output:
        recolor_done_fn = recolor_done_fmt
    run:
        czi_fns = input.czi_fns
        # print(czi_fns)
        # Get bc sciname dict
        pdfn, bc_type = dict_date_pdfn[wildcards.date]
        probe_design = pd.read_csv(pdfn)
        barcodes = probe_design['code']
        sci_names = probe_design['sci_name']
        dict_bc_sciname = dict(zip(barcodes, sci_names))
        # # filenames
        # mlab_fn = mlab_fmt.format(
        #     date=wildcards.date, sn=wildcards.sn
        # )
        # clust_fn = clust_fmt.format(
        #     date=wildcards.date, sn=wildcards.sn
        # )
        # classif_fn = classif_fmt.format(
        #     date=wildcards.date, sn=wildcards.sn
        # )

        # props
        prop_glob = props_fmt.format(
            date=wildcards.date, sn=wildcards.sn, m='*'
        )
        prop_fns = glob.glob(prop_glob)
        Ms = [int(re.search('(?<=_M_)\d+',fn)[0]) for fn in prop_fns]
        prop_fns = [x for _, x in sorted(zip(Ms, prop_fns))]

        # segs
        seg_glob = seg_fmt.format(date=wildcards.date, sn=wildcards.sn, m='*')
        seg_fns = glob.glob(seg_glob)
        Ms = [int(re.search('(?<=_M_)\d+',fn)[0]) for fn in seg_fns]
        seg_fns = [x for _, x in sorted(zip(Ms, seg_fns))]

        # Get number of tiles or scenes
        M, mtype = fsi.get_ntiles(input.czi_fns[0])
        # Get the resolutions
        resolutions = [fsi.get_resolution(fn) for fn in input.czi_fns]
        # Get the lasers
        lasers = [fsi.get_laser(fn) for fn in input.czi_fns]
        # remove 405 channel
        czi_fns = [fn for fn, l in zip(input.czi_fns, lasers) if l != 405]
        resolutions = [r for r, l in zip(resolutions, lasers) if l != 405]
        lasers = [l for l in lasers if l != 405]
        czi_fns = [x for _, x in sorted(zip(lasers, czi_fns))]
        resolutions = [x for _, x in sorted(zip(lasers, resolutions))]
        lasers = sorted(lasers)

        # # Load mlab dict
        # with open(mlab_fn, 'r') as f:
        #     dict_m_lab_ind = yaml.unsafe_load(f)
        # # Load clusters
        # clust_agg = np.load(clust_fn)
        # cl_unq = np.unique(clust_agg)
        # # Load classif
        # with open(classif_fn, 'r') as f:
        #     dict_cl_bc = yaml.unsafe_load(f)

        # Get upper left corners of tiles if it is a tilescan
        # M, mtype = fsi.get_ntiles(czi_fns[0])
        # res_umpix = fsi.get_resolution(czi_fns[0]) * 1e6
        res_umpix = resolutions[0] * 1e6
        if mtype == 'M':
            ncols = int(fsi.get_metadata_value(czi_fns[0], 'TilesX')[0])
            nrows = int(fsi.get_metadata_value(czi_fns[0], 'TilesY')[0])
            overl = float(fsi.get_metadata_value(
                czi_fns[0], 'TileAcquisitionOverlap'
            )[0])
            cols = np.tile(np.arange(ncols), nrows)
            rows = np.repeat(np.arange(nrows), ncols)
            seg = np.load(seg_fns[0])
            shp = np.array(seg.shape[:2])
            im_r, im_c = shp - shp * overl
            ul_corners = []
            for m in range(M):
                c, r = cols[m], rows[m]
                ulc = np.array([r * im_r, c * im_c])
                ul_corners.append(ulc)

            # Plot classified tiled image
            tile_shp = (
                (nrows - 1)*int(math.ceil(im_r)) + shp[0], 
                (ncols - 1)*int(math.ceil(im_c)) + shp[1]
            )
            classif_tile = np.zeros((
                tile_shp[0], tile_shp[1],len(colors[0])
            ))
            sum_tile = np.zeros((tile_shp[0], tile_shp[1]))
            # Get cell absolute locations
            dict_ind_centroid_sciname = {}
            ul_corners = np.array(ul_corners)
            r_lims = np.unique(ul_corners[:,0])
            c_lims = np.unique(ul_corners[:,1])

        # Get normalization info from tiles
        min_global = 1e10
        max_global = 0
        mstd_global = 0
        mstd1_5_global = 0
        mstd2_global = 0
        for m in range(M):
            # Get shifts
            try:
                raws = [fsi.load_raw(fn, m, mtype) for fn in czi_fns]
                raws = [fsi.reshape_aics_image(r) for r in raws]
            except:
                raws = [np.zeros_like(r) for r in raws]
            # raws = [fsi.load_raw(fn, m, mtype) for fn in czi_fns]
            # raws = [fsi.reshape_aics_image(r) for r in raws]
            # some images have different pixel resolution, correct that
            raws = fsi.match_resolutions_and_size(raws, resolutions)
            image_max_norm = [fsi.max_norm(r) for r in raws]
            sh = fsi._get_shift_vectors(image_max_norm)
            raws_shift = fsi._shift_images(
                raws, sh, max_shift=config['max_shift']
            )            
            stack = np.dstack(raws_shift)
            stack_sum = np.sum(stack, axis=2)
            min_m = np.min(stack_sum)
            mean_m = np.mean(stack_sum)
            std_m = np.std(stack_sum)
            mstd = mean_m + std_m
            mstd1_5 = mean_m + 1.5*std_m
            mstd2 = mean_m + 2*std_m
            max_m = np.max(stack_sum)
            min_global = min_m if min_m < min_global else min_global
            max_global = max_m if max_m > max_global else max_global
            mstd_global = mstd if mstd > mstd_global else mstd_global
            mstd1_5_global = mstd1_5 if mstd1_5 > mstd1_5_global else mstd1_5_global
            mstd2_global = mstd2 if mstd2 > mstd2_global else mstd2_global


        # Plot each FOV
        classif_all = []
        color_scaling = ['1','1_5','2']
        mstds = [mstd_global, mstd1_5_global, mstd2_global]
        for cscale, mxg in zip(color_scaling, mstds):        
            for m in range(M):
                # Get shifts
                try:
                    raws = [fsi.load_raw(fn, m, mtype) for fn in czi_fns]
                    raws = [fsi.reshape_aics_image(r) for r in raws]
                except:
                    raws = [np.zeros_like(r) for r in raws]
                # raws = [fsi.load_raw(fn, m, mtype) for fn in czi_fns]
                # raws = [fsi.reshape_aics_image(r) for r in raws]
                # some images have different pixel resolution, correct that
                if np.max([np.max(r) for r in raws]) > 0:
                    raws = fsi.match_resolutions_and_size(raws, resolutions)
                    image_max_norm = [fsi.max_norm(r) for r in raws]
                    sh = fsi._get_shift_vectors(image_max_norm)
                    raws_shift = fsi._shift_images(
                        raws, sh, max_shift=config['max_shift']
                    )

                    # Segment with no edge correction
                    stack = np.dstack(raws_shift)
                    stack_sum = np.sum(stack, axis=2)
                    pre = sf.pre_process(
                        stack_sum, 
                        gauss=config['gauss'], 
                        diff_gauss=eval(config['diff_gauss'])
                        )
                    mask = sf.get_background_mask(
                        stack_sum,
                        bg_smoothing=config['bg_smoothing'],
                        n_clust_bg=config['n_clust_bg'],
                        top_n_clust_bg=config['top_n_clust_bg'],
                    )
                    seg = sf.segment_02(pre, mask)
                    props = sf.measure_regionprops(seg, stack_sum)

                    # load old props 
                    props_fn_old = props_fmt.format(
                        date=wildcards.date, sn=wildcards.sn, m=m
                    )
                    props_old = pd.read_csv(props_fn_old)

                    # Load scinames
                    classif_fn = classif_fmt.format(
                        date=wildcards.date, sn=wildcards.sn, m=m
                    )
                    scinames_cosdist = np.load(classif_fn)

                    # map new seg to old
                    centroids = props_old.centroid.values
                    dict_nlab_color = {}
                    for i, c in enumerate(centroids):
                        c = eval(c)
                        c = [int(c_) for c_ in c]
                        sciname = scinames_cosdist[i]
                        color = dict_sciname_color[sciname]
                        lnew = seg[c[0],c[1]]
                        if lnew:
                            dict_nlab_color[lnew] = color

                    # Adjust raw image
                    # stack_sum_norm = fsi.max_norm(stack, type='sum')
                    stack_sum_norm = (stack_sum - min_global) / (mxg - min_global)
                    try:
                        sn_ = wildcards.date + '_' + wildcards.sn
                        # clips = eval(config['clip_dict'][sn_])
                        clips = clip_dict[sn_]
                    except:
                        clips = eval(config['clips'])
                    stack_sum_norm = np.clip(stack_sum_norm, clips[0], clips[1])
                    stack_sum_norm = (stack_sum_norm - clips[0])/(clips[1] - clips[0])
                    # stack_sum_norm = fsi.max_norm(stack, type='sum', c=eval(config['clips']))

                    # Plot classif
                    dict_lab_bbox = dict(zip(props.label.values,props.bbox.values))
                    dict_nlab_bbox = {l: dict_lab_bbox[l] for l in dict_nlab_color.keys()}
                    seg_classif = sf.seg_2_rgb_dict(
                        seg, 
                        dict_nlab_color, 
                        dict_nlab_bbox, 
                        raw=stack_sum_norm
                    )
                else:
                    seg_classif = np.zeros(raws[0].shape[:2] + (3,))

                classif_all.append(seg_classif)
                
                # Save plot
                fig, ax, cbar = ip.plot_image(
                    seg_classif, 
                    im_inches=config['im_inches'], 
                    scalebar_resolution=res_umpix,
                    cb_col=config['cb_col'],
                    ft=config['im_ft']
                )
                classif_plot_fn = plot_recolor_fmt.format(
                    date=wildcards.date, sn=wildcards.sn, m=m, cscale=cscale
                )
                d = os.path.split(classif_plot_fn)[0]
                if not os.path.exists(d):
                    os.makedirs(d)
                    print('Made dir:', d)
                plt.figure(fig)
                ip.save_fig(classif_plot_fn, dpi=config['dpi'], bbox_inches=0)   
                plt.close()

                # overlap tiles if it is a tilescan
                if mtype == 'M':
                    # Get image corner values
                    c, r = cols[m], rows[m]
                    ulc = ul_corners[m]
                    # Get limits to remove cells from overlap
                    r_lim, c_lim = [1e15]*2
                    if r < np.max(rows):
                        r_lim = r_lims[r+1]
                    if c < np.max(cols):
                        c_lim = c_lims[c+1]
                        
                    # Write to all tiles image
                    cr_shp = seg_classif.shape[:2]
                    ulc = [int(c) for c in ulc]
                    # print(ulc, cr_shp)
                    classif_tile[ulc[0]:ulc[0] + cr_shp[0], ulc[1]:ulc[1] + cr_shp[1], :] = seg_classif


            # Plot all tiles together if it is a tilescan
            if mtype == 'M':
                fig, ax, cbar = ip.plot_image(
                    classif_tile, 
                    im_inches=config['im_inches'], 
                    scalebar_resolution=res_umpix,
                    cb_col=config['cb_col'],
                    ft=config['im_ft']
                )
                plot_recolor_all_fn = plot_recolor_all_fmt.format(
                    date=wildcards.date, sn=wildcards.sn, cscale=cscale
                )
                d = os.path.split(plot_recolor_all_fn)[0]
                if not os.path.exists(d):
                    os.makedirs(d)
                    print('Made dir:', d)
                plt.figure(fig)
                ip.save_fig(plot_recolor_all_fn, dpi=config['dpi'], bbox_inches=0)
                plt.close()
            else:
                for m in range(M):
                    fig, ax, cbar = ip.plot_image(
                        classif_all[m], 
                        im_inches=config['im_inches'], 
                        scalebar_resolution=res_umpix,
                        cb_col=config['cb_col'],
                        ft=config['im_ft']
                    )
                        # im_inches=config['im_inches']*np.max(rows), 
                    plot_recolor_all_fn = plot_recolor_all_fmt.format(
                        date=wildcards.date, sn=wildcards.sn, cscale=cscale
                    )
                    ext = '_M_{}.png'.format(m)
                    plot_recolor_all_fn = re.sub('.png',ext, plot_recolor_all_fn)
                    d = os.path.split(plot_recolor_all_fn)[0]
                    if not os.path.exists(d):
                        os.makedirs(d)
                        print('Made dir:', d)
                    plt.figure(fig)
                    ip.save_fig(plot_recolor_all_fn, dpi=config['dpi'], bbox_inches=0)
                    plt.close()
        
        with open(output.recolor_done_fn, 'w') as f:
            f.write('Snakemake rule recolor done')


rule color_legend:
    input:
        recolor_done
    output:
        color_legend_fn = color_legend_fn
    run:
        fig, ax = ip.taxon_legend(
            sciname_list, 
            colors, 
            label_color=config['color_legend']['label_color'],
            ft=config['color_legend']['ft'],
            dims=eval(config['color_legend']['dims']),
        )
        ip.save_fig(output.color_legend_fn)

rule rgb_tile:
    input:
        czi_fns = lambda wildcards: get_czi_fns(f'{wildcards.date}',f'{wildcards.sn}'),
    output:
        rgb_tile_done = rgb_tile_done_fmt
    run:
        czi_fns = input.czi_fns

        # Get number of tiles or scenes
        M, mtype = fsi.get_ntiles(input.czi_fns[0])
        # Get the resolutions
        resolutions = [fsi.get_resolution(fn) for fn in input.czi_fns]
        # Get the lasers
        lasers = [fsi.get_laser(fn) for fn in input.czi_fns]
        # remove 405 channel
        czi_fns = [fn for fn, l in zip(input.czi_fns, lasers) if l != 405]
        resolutions = [r for r, l in zip(resolutions, lasers) if l != 405]
        lasers = [l for l in lasers if l != 405]
        czi_fns = [x for _, x in sorted(zip(lasers, czi_fns))]
        resolutions = [x for _, x in sorted(zip(lasers, resolutions))]
        lasers = sorted(lasers)

        # Get upper left corners of tiles if it is a tilescan
        res_umpix = resolutions[0] * 1e6
        if mtype == 'M':
            ncols = int(fsi.get_metadata_value(czi_fns[0], 'TilesX')[0])
            nrows = int(fsi.get_metadata_value(czi_fns[0], 'TilesY')[0])
            overl = float(fsi.get_metadata_value(
                czi_fns[0], 'TileAcquisitionOverlap'
            )[0])
            cols = np.tile(np.arange(ncols), nrows)
            rows = np.repeat(np.arange(nrows), ncols)
            raw = fsi.load_raw(czi_fns[0], 0, mtype)
            raw = fsi.reshape_aics_image(raw)
            shp = np.array(raw.shape[:2])
            im_r, im_c = shp - shp * overl
            ul_corners = []
            for m in range(M):
                c, r = cols[m], rows[m]
                ulc = np.array([r * im_r, c * im_c])
                ul_corners.append(ulc)

            # Plot classified tiled image
            tile_shp = (
                (nrows - 1)*int(math.ceil(im_r)) + shp[0], 
                (ncols - 1)*int(math.ceil(im_c)) + shp[1]
            )
            rgb_tile = np.zeros((
                tile_shp[0], tile_shp[1], 3
            ))
            # Get cell absolute locations
            ul_corners = np.array(ul_corners)
            r_lims = np.unique(ul_corners[:,0])
            c_lims = np.unique(ul_corners[:,1])

        # Get normalization info from tiles
        min_global = [1e10]*3
        max_global = [0]*3
        for m in range(M):
            # Get shifts
            raws = [fsi.load_raw(fn, m, mtype) for fn in czi_fns]
            raws = [fsi.reshape_aics_image(r) for r in raws]
            # some images have different pixel resolution, correct that
            raws = fsi.match_resolutions_and_size(raws, resolutions)
            image_max_norm = [fsi.max_norm(r) for r in raws]
            sh = fsi._get_shift_vectors(image_max_norm)
            raws_shift = fsi._shift_images(
                raws, sh, max_shift=config['max_shift']
            )            
            sum_shift = [np.sum(r, axis=2) for r in raws_shift]
            min_m = [np.min(s) for s in sum_shift]
            max_m = [np.max(s) for s in sum_shift]
            for i, (mg, mm) in enumerate(zip(min_global, min_m)):
                min_global[i] = mm if mm < mg else mg
            for i, (mg, mm) in enumerate(zip(max_global, max_m)):
                max_global[i] = mm if mm > mg else mg
            # max_global = max_m if max_m > max_global else max_global


        # Combine each FOV
        sum_clip_stack_all = []
        for m in range(M):
            # Get shifts
            raws = [fsi.load_raw(fn, m, mtype) for fn in czi_fns]
            raws = [fsi.reshape_aics_image(r) for r in raws]
            # some images have different pixel resolution, correct that
            raws = fsi.match_resolutions_and_size(raws, resolutions)
            image_max_norm = [fsi.max_norm(r) for r in raws]
            sh = fsi._get_shift_vectors(image_max_norm)
            raws_shift = fsi._shift_images(
                raws, sh, max_shift=config['max_shift']
            )

            # Adjust raw image
            sum_shift = [np.sum(r, axis=2) for r in raws_shift]
            sum_norm = []
            for ss, mn, mx in zip(sum_shift, min_global, max_global):
                sum_norm.append((ss - mn) / (mx - mn))
            # stack_sum_norm = fsi.max_norm(stack, type='sum')
            # stack_sum_norm = (stack_sum - min_global) / (max_global - min_global)
            try:
                sn_ = wildcards.date + '_' + wildcards.sn
                # clips = eval(config['clip_dict'][sn_])
                clips = rgb_clip_dict[sn_]
            except:
                clips = config['rgb_clips']

            sum_clip = []
            for snrm, clp in zip(sum_norm, clips):
                sclp = np.clip(snrm, clp[0], clp[1])
                sum_clip.append((sclp - clp[0]) / (clp[1] - clp[0]))
            # stack_sum_norm = np.clip(stack_sum_norm, clips[0], clips[1])
            # stack_sum_norm = (stack_sum_norm - clips[0])/(clips[1] - clips[0])
            # stack_sum_norm = fsi.max_norm(stack, type='sum', c=eval(config['clips']))

            sum_clip_stack = np.dstack(sum_clip)
            sum_clip_stack_all.append(sum_clip_stack)

            # overlap tiles if it is a tilescan
            if mtype == 'M':
                # Get image corner values
                c, r = cols[m], rows[m]
                ulc = ul_corners[m]
                # Get limits to remove cells from overlap
                r_lim, c_lim = [1e15]*2
                if r < np.max(rows):
                    r_lim = r_lims[r+1]
                if c < np.max(cols):
                    c_lim = c_lims[c+1]
                    
                # Write to all tiles image
                cr_shp = sum_clip[0].shape[:2]
                ulc = [int(c) for c in ulc]
                # print(ulc, cr_shp)
                rgb_tile[ulc[0]:ulc[0] + cr_shp[0], ulc[1]:ulc[1] + cr_shp[1], :] = sum_clip_stack


        # Plot all tiles together if it is a tilescan
        if mtype == 'M':
            fig, ax, cbar = ip.plot_image(
                rgb_tile, 
                im_inches=config['im_inches'], 
                scalebar_resolution=res_umpix,
                cb_col=config['cb_col'],
                ft=config['im_ft']
            )
                # im_inches=config['im_inches']*np.max(rows), 
            plot_rgb_tile_fn = plot_rgb_tile_fmt.format(
                date=wildcards.date, sn=wildcards.sn
            )
            d = os.path.split(plot_rgb_tile_fn)[0]
            if not os.path.exists(d):
                os.makedirs(d)
                print('Made dir:', d)
            plt.figure(fig)
            ip.save_fig(plot_rgb_tile_fn, dpi=config['dpi'], bbox_inches=0)
            plt.close()
        else:
            for m in range(M):
                fig, ax, cbar = ip.plot_image(
                    sum_clip_stack_all[m], 
                    im_inches=config['im_inches'], 
                    scalebar_resolution=res_umpix,
                    cb_col=config['cb_col'],
                    ft=config['im_ft']
                )
                    # im_inches=config['im_inches']*np.max(rows), 
                plot_rgb_tile_fn = plot_rgb_tile_fmt.format(
                    date=wildcards.date, sn=wildcards.sn
                )
                ext = '_M_{}.png'.format(m)
                plot_rgb_tile_fn = re.sub('.png',ext, plot_rgb_tile_fn)
                d = os.path.split(plot_rgb_tile_fn)[0]
                if not os.path.exists(d):
                    os.makedirs(d)
                    print('Made dir:', d)
                plt.figure(fig)
                ip.save_fig(plot_rgb_tile_fn, dpi=config['dpi'], bbox_inches=0)
                plt.close()
        
        with open(output.rgb_tile_done, 'w') as f:
            f.write('Snakemake rule rgb_tile done')

rule get_abundances:
    input:
        classif_done_fn = classif_done_fmt,
    output:
        abundances_fn = abundances_fmt
    run:
        # print(czi_fns)
        # Get bc sciname dict
        pdfn, bc_type = dict_date_pdfn[wildcards.date]
        probe_design = pd.read_csv(pdfn)
        barcodes = probe_design['code']
        sci_names = probe_design['sci_name']
        dict_bc_sciname = dict(zip(barcodes, sci_names))

        # Load clusters
        clust_agg = np.load(input.clust_fn)
        cl_unq = np.unique(clust_agg)
        # Load classif
        with open(input.classif_fn, 'r') as f:
            dict_cl_bc = yaml.unsafe_load(f)

        # Get scinames
        scinames = []
        for cl in clust_agg:
            bc = dict_cl_bc[cl]
            scinames.append(dict_bc_sciname[bc])
        
        # Save 
        scinames = np.array(scinames)
        np.save(output.abundances_fn, scinames)     




